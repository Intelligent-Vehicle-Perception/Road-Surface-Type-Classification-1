{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Deep Learning - Processing.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"m3WpWEwwlV5K"},"source":["<h1>Deep Learning - Processing</h1>"]},{"cell_type":"markdown","metadata":{"id":"nR00SdqulV5S"},"source":["<h5>Data Parameters</h5>"]},{"cell_type":"code","metadata":{"id":"y6Jrok4nlV5T"},"source":["try:\n","    from google.colab import output\n","    IN_COLAB = True\n","except:\n","    output = None\n","    IN_COLAB = False\n","\n","if IN_COLAB:\n","    datasets_folder = '/drive/My Drive/Colab Notebooks/DataSets/'\n","    work_folder = '/drive/My Drive/Colab Notebooks/Experiments/Deep Learning/'\n","else:\n","    datasets_folder = '/Google Drive/Colab Notebooks/DataSets/'\n","    work_folder = '/Google Drive/Colab Notebooks/Experiments/Deep Learning/'\n","\n","print(\"In Colab:\", IN_COLAB)\n","print(\"Work Folder:\", work_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qBLe_2hXlV5M"},"source":["<h5>Importing Packages</h5>"]},{"cell_type":"code","metadata":{"id":"hEH4jRqFsEcp"},"source":["try:\n","    import livelossplot\n","except:\n","    !pip install livelossplot --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FkhkhXHlV5O"},"source":["import os\n","import gc\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from IPython.display import display\n","from tqdm.notebook import tqdm\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler\n","from sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix, classification_report\n","import seaborn as sns\n","# pd.set_option(\"float_format\", '{:0.10f}'.format)\n","# pd.set_option('display.max_columns', 30) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7kPsJqpX-GD"},"source":["# -------------------------------------\n","# TensorFlow/Keras Selection\n","# -------------------------------------\n","\n","# useKerasTfV1=False\n","# useKerasTfV2=False\n","# useTfV2=True\n","\n","if useKerasTfV1:\n","\n","    if IN_COLAB:\n","        %tensorflow_version 1.x\n","    \n","    import tensorflow as tf\n","    from tensorflow.python.util import deprecation\n","    deprecation._PRINT_DEPRECATION_WARNINGS = False\n","    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","    tf.logging.set_verbosity(tf.logging.ERROR)\n","    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","    \n","    import keras\n","    from keras.models import Sequential\n","    from keras.callbacks import History, ModelCheckpoint, CSVLogger, EarlyStopping\n","    from keras.layers import Activation, Dense, Dropout, SpatialDropout1D, Conv1D, TimeDistributed, MaxPooling1D, Flatten, ConvLSTM2D, Bidirectional, BatchNormalization, GlobalAvgPool1D, GlobalAveragePooling1D, MaxPooling1D, CuDNNLSTM as LSTM\n","    from keras.utils import plot_model\n","    \n","    from livelossplot import PlotLossesKeras\n","\n","    print(\"Using Tensorflow\", tf.__version__)\n","    print(\"Using Keras\", keras.__version__)\n","\n","if useKerasTfV2:\n","    \n","    import tensorflow as tf\n","    \n","    import keras\n","    from keras.models import Sequential\n","    from keras.callbacks import History, ModelCheckpoint, CSVLogger, EarlyStopping\n","    from keras.layers import Activation, Dense, Dropout, SpatialDropout1D, Conv1D, TimeDistributed, MaxPooling1D, Flatten, ConvLSTM2D, Bidirectional, BatchNormalization, GlobalAvgPool1D, GlobalAveragePooling1D, MaxPooling1D, LSTM\n","    from keras.utils import plot_model\n","    \n","    from livelossplot import PlotLossesKerasTF\n","    \n","    print(\"Using Tensorflow\", tf.__version__)\n","    print(\"Using Keras\", keras.__version__)\n","\n","    if not(keras.__version__.startswith(\"2.3\")):\n","        !pip install \"keras>=2.3.0\"\n","\n","if useTfV2:\n","    import tensorflow as tf\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.callbacks import History, ModelCheckpoint, CSVLogger, EarlyStopping\n","    from tensorflow.keras.layers import Activation, Dense, Dropout, SpatialDropout1D, Conv1D, TimeDistributed, MaxPooling1D, Flatten, ConvLSTM2D, Bidirectional, BatchNormalization, GlobalAvgPool1D, GlobalAveragePooling1D, MaxPooling1D, LSTM\n","    from tensorflow.keras.utils import plot_model\n","    \n","    from livelossplot import PlotLossesKerasTF\n","    \n","    print(\"Using Tensorflow\", tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JB_YhW184c2I"},"source":["# print(\"Devices Tensorflow\")\n","# print(tf.config.list_physical_devices())\n","# print(tf.compat.v1.config.list_physical_devices())\n","# from tensorflow.python.client import device_lib \n","# print(device_lib.list_local_devices())\n","# print(\"Devices Nvidia\")\n","# !nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LYpqWHmElV5X"},"source":["<h5>Data Functions</h5>"]},{"cell_type":"code","metadata":{"id":"eMcwk8WrlV5Y"},"source":["# Load raw datasets from disk\n","# Input: folder where is the datasets folder and files\n","# Output: dict -> { \"pvs_x\": { \"left\": df, \"right\": df, \"labels\": df } }\n","def getDataSets(folder=datasets_folder):\n","    \n","    datasets = {}\n","    \n","    for i in range(1, 10):\n","        \n","        dataset_folder = os.path.join(folder, \"PVS \" + str(i))\n","        \n","        left =   pd.read_csv(os.path.join(dataset_folder, 'dataset_gps_mpu_left.csv'),  float_precision=\"high\")\n","        right =  pd.read_csv(os.path.join(dataset_folder, 'dataset_gps_mpu_right.csv'), float_precision=\"high\")\n","        labels = pd.read_csv(os.path.join(dataset_folder, 'dataset_labels.csv'),        float_precision=\"high\")\n","        \n","        datasets[\"pvs_\" + str(i)] = {\n","            \"left\": left,\n","            \"right\": right,\n","            \"labels\": labels\n","        }\n","    \n","    return datasets\n","\n","# Get fields filtering by inputs\n","# Input: data types and placements\n","# Output: string[]\n","def getFields(acc=False, gyro=False, mag=False, temp=False, speed=False, location=False, below_suspension=False, above_suspension=False, dashboard=False):\n","    \n","    all_fields = [\n","        'timestamp', \n","        'acc_x_dashboard', 'acc_y_dashboard', 'acc_z_dashboard',\n","        'acc_x_above_suspension', 'acc_y_above_suspension', 'acc_z_above_suspension', \n","        'acc_x_below_suspension', 'acc_y_below_suspension', 'acc_z_below_suspension', \n","        'gyro_x_dashboard', 'gyro_y_dashboard', 'gyro_z_dashboard', \n","        'gyro_x_above_suspension', 'gyro_y_above_suspension', 'gyro_z_above_suspension',\n","        'gyro_x_below_suspension', 'gyro_y_below_suspension', 'gyro_z_below_suspension', \n","        'mag_x_dashboard', 'mag_y_dashboard', 'mag_z_dashboard', \n","        'mag_x_above_suspension', 'mag_y_above_suspension', 'mag_z_above_suspension', \n","        'temp_dashboard', 'temp_above_suspension', 'temp_below_suspension', \n","        'timestamp_gps', 'latitude', 'longitude', 'speed'\n","    ]\n","    \n","    return_fields = []\n","    \n","    for field in all_fields:\n","            \n","        data_type = False\n","        placement = False\n","        \n","        if(speed and field == \"speed\"):\n","            placement = data_type = True\n","            \n","        if(location and (field == \"latitude\" or field == \"longitude\")):\n","            placement = data_type = True\n","        \n","        if(acc):\n","            data_type = data_type or field.startswith(\"acc_\")\n","        \n","        if(gyro):\n","            data_type = data_type or field.startswith(\"gyro_\")\n","            \n","        if(mag):\n","            data_type = data_type or field.startswith(\"mag_\")\n","            \n","        if(temp):\n","            data_type = data_type or field.startswith(\"temp_\")\n","            \n","        if(below_suspension):\n","            placement = placement or field.endswith(\"_below_suspension\")\n","            \n","        if(above_suspension):\n","            placement = placement or field.endswith(\"_above_suspension\")\n","            \n","        if(dashboard):\n","            placement = placement or field.endswith(\"_dashboard\")\n","        \n","        if(data_type and placement):\n","            return_fields.append(field)\n","            \n","    return return_fields\n","\n","# Get subsets from raw datasets. \n","# For each raw dataset, returns a subset with only fields passed.\n","# Input: raw datasets (dict), fields (string[]) and labels (string[])\n","# Output: dict -> { \"pvs_x\": { \"left\": df, \"right\": df, \"labels\": df } }\n","def getSubSets(datasets, fields, labels):\n","    \n","    subsets = {}\n","    \n","    for key in datasets.keys():\n","        \n","        subsets[key] = {\n","            \"left\": datasets[key][\"left\"][fields],\n","            \"right\": datasets[key][\"right\"][fields],\n","            \"labels\": datasets[key][\"labels\"][labels]\n","        }\n","    \n","    return subsets\n","\n","# Get normalized data.\n","# Input: subsets (dict).\n","# Output: dict -> { \"pvs_x\": { \"left\": df, \"right\": df, \"labels\": df } }\n","def getNormalizedDataMinMax(subsets, scaler_range):\n","    \n","    normalized_sets = {}\n","    learn_data = pd.DataFrame()\n","\n","    for pvs in subsets.keys():\n","        for side in [\"left\", \"right\"]:\n","            learn_data = learn_data.append(subsets[pvs][side], ignore_index=True)\n","        \n","    # Normalize between scaler_min and scaler_max\n","    scaler = MinMaxScaler(feature_range=scaler_range)\n","    scaler.fit(learn_data)\n","    \n","    for pvs in subsets.keys():\n","        \n","        normalized_sets[pvs] = {\n","            'left':  pd.DataFrame(data=scaler.transform(subsets[pvs]['left']),  columns=subsets[pvs]['left'].columns),\n","            'right': pd.DataFrame(data=scaler.transform(subsets[pvs]['right']), columns=subsets[pvs]['right'].columns),\n","            'labels': subsets[pvs]['labels']\n","        }\n","                    \n","    return scaler, normalized_sets \n","\n","# Get standardized data.\n","# Input: subsets (dict).\n","# Output: dict -> { \"pvs_x\": { \"left\": df, \"right\": df, \"labels\": df } }\n","def getNormalizedDataRobust(subsets):\n","    \n","    normalized_sets = {}\n","    learn_data = pd.DataFrame()\n","\n","    for pvs in subsets.keys():\n","        for side in [\"left\", \"right\"]:\n","            learn_data = learn_data.append(subsets[pvs][side], ignore_index=True)\n","        \n","    # Standardize\n","    scaler = RobustScaler()\n","    scaler = scaler.fit(learn_data)\n","    \n","    for pvs in subsets.keys():\n","        \n","        normalized_sets[pvs] = {\n","            'left':  pd.DataFrame(data=scaler.transform(subsets[pvs]['left']),  columns=subsets[pvs]['left'].columns),\n","            'right': pd.DataFrame(data=scaler.transform(subsets[pvs]['right']), columns=subsets[pvs]['right'].columns),\n","            'labels': subsets[pvs]['labels']\n","        }\n","                    \n","    return scaler, normalized_sets \n","\n","# Inputs and outputs -> np.array with lines x columns (n samples x values/variables/features).\n","# moving_window: if window is moving\n","# shape: (None, ..., ..., n variables)\n","# Mode label: most common value in window, else will be used last label, value at last position in window\n","def reshapeData(inputs, outputs, shape, moving_window, mode_label):  \n","\n","    shape = tuple([x for x in shape if x is not None])\n","\n","    window = 1\n","\n","    for dim in shape:\n","        window = window * dim\n","\n","    window = int(window / len(inputs[0]))\n","\n","    if moving_window:\n","\n","        inputs_reshaped = []\n","\n","        if mode_label:\n","            outputs_reshaped = []\n","        else:\n","            outputs_reshaped = outputs[window-1:]\n","\n","        for i in range(window, len(inputs)+1):\n","            value = inputs[i-window:i, :]\n","            value = value.reshape(shape)\n","            inputs_reshaped.append(value)\n","\n","            if mode_label:\n","                outputs_reshaped.append(outputs[i-window:i, :].mean(axis=0).round(0))\n","\n","    else:\n","\n","        inputs_reshaped = []\n","        outputs_reshaped = []\n","\n","        chuncks = int(len(inputs)/window)\n","\n","        for i in range(0, chuncks):\n","            value = inputs[i*window : (i+1)*window, :]\n","            value = value.reshape(shape)\n","            inputs_reshaped.append(value)\n","\n","            if mode_label:\n","                outputs_reshaped.append(outputs[i*window : (i+1)*window, :].mean(axis=0).round(0))\n","            else:\n","                outputs_reshaped.append(outputs[((i+1)*window)-1])\n","\n","    return inputs_reshaped, outputs_reshaped\n","\n","# Get train and test sets from normalized sets.\n","# Inputs: normalized sets (dict), sets_train (string[]) and sets_test (string[]) with datasets names (\"pvs_x\"), shape to reshape data, window type and sides.\n","# Outputs: df -> input train, input test, output train, output test\n","def getTrainTestSets(normalized_sets, sets_train, sets_test, shape, moving_window=True, sides=['left', 'right'], mode_label=False):\n","\n","    input_train = []\n","    input_test = []\n","    output_train = []\n","    output_test = []\n","\n","    for key in normalized_sets.keys():\n","\n","        for side in sides:\n","\n","            inputs, outputs = reshapeData(normalized_sets[key][side].values, normalized_sets[key][\"labels\"].values, shape, moving_window, mode_label)\n","        \n","            if (key in sets_train):\n","\n","                for inp in inputs:\n","                    input_train.append(inp)\n","\n","                for out in outputs:\n","                    output_train.append(out)             \n","              \n","            elif (key in sets_test):\n","\n","                for inp in inputs:\n","                    input_test.append(inp)\n","\n","                for out in outputs:\n","                    output_test.append(out) \n","\n","    return input_train, input_test, output_train, output_test\n","\n","def createMemoryMap(path, input_train, input_test, output_train, output_test):\n","\n","    it_file = os.path.join(path, 'input_train.dat')\n","    it_map = np.memmap(it_file, dtype='float64', mode='w+', shape=((len(input_train),) + input_train[0].shape))\n","    it_map[:] = input_train[:]\n","\n","    iv_file = os.path.join(path, 'input_test.dat')\n","    iv_map = np.memmap(iv_file, dtype='float64', mode='w+', shape=((len(input_test),) + input_test[0].shape))\n","    iv_map[:] = input_test[:]\n","\n","    ot_file = os.path.join(path, 'output_train.dat')\n","    ot_map = np.memmap(ot_file, dtype='float64', mode='w+', shape=((len(output_train),) + output_train[0].shape))\n","    ot_map[:] = output_train[:]\n","\n","    ov_file = os.path.join(path, 'output_test.dat')\n","    ov_map = np.memmap(ov_file, dtype='float64', mode='w+', shape=((len(output_test),) + output_test[0].shape))\n","    ov_map[:] = output_test[:]\n","\n","    return it_map, iv_map, ot_map, ov_map"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvAuMxbDlV5b"},"source":["<h5>Model Management</h5>"]},{"cell_type":"code","metadata":{"id":"WU14cNYZlV5c"},"source":["def modelFileSavedFormat(file):\n","    return file + '-train-acc-{acc:.5f}-val-acc-{val_acc:.5f}.hdf5'\n","\n","def createPathIfNotExists(path):\n","\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","def saveModelDiagram(model, path, file, show=True):\n","    createPathIfNotExists(path)\n","    plot_model(model, to_file=os.path.join(path, file + '.png'), show_shapes=True, show_layer_names=True)\n","\n","    if show:\n","        display(plot_model(model, show_shapes=True, show_layer_names=True))\n","        # print(model.summary())\n","    \n","def showHistory(history):\n","    \n","    for key in history.history.keys():\n","        plt.plot(history.history[key], label=key)\n","    \n","    plt.legend()\n","    \n","def fitModel(model, inputs_train, outputs_train, inputs_validation, outputs_validation, path, file, batch_size=64, epochs=1000, patience=10):\n","    \n","    createPathIfNotExists(path)\n","    \n","    # train_folder = os.path.join(path, 'train')\n","    # createPathIfNotExists(train_folder)\n","    # checkpoint_file_train = os.path.join(train_folder, 'checkpoint-train-{epoch:002d}-{loss:.10f}-{acc:.5f}-val-{val_loss:.10f}-{val_acc:.5f}.hdf5')\n","    # checkpoint_train = ModelCheckpoint(filepath=checkpoint_file_train, save_best_only=True, monitor='acc', mode='max')\n","\n","    # validation_folder = os.path.join(path, 'validation')\n","    # createPathIfNotExists(validation_folder)\n","    # checkpoint_file_validation = os.path.join(validation_folder, 'checkpoint-{epoch:002d}-train-{loss:.10f}-{acc:.5f}-val-{val_loss:.10f}-{val_acc:.5f}.hdf5')\n","    # checkpoint_validation = ModelCheckpoint(filepath=checkpoint_file_validation, save_best_only=True, monitor='val_acc', mode='max', verbose=1)\n","\n","    checkpoint_file_validation = os.path.join(path, modelFileSavedFormat(file))\n","    checkpoint_validation = ModelCheckpoint(filepath=checkpoint_file_validation, save_best_only=True, monitor='val_acc', mode='max') # verbose=1\n","    \n","    # logger_file = os.path.join(path, file + '-training-log.csv')\n","    # csv_logger = CSVLogger(logger_file, append=True)\n","    \n","    plotlosses = PlotLossesKeras()\n","    \n","    early_stopping = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=patience, min_delta=0.0001, restore_best_weights=True)\n","    \n","    # callbacks=[csv_logger, checkpoint_train, checkpoint_validation, early_stopping]\n","    callbacks=[plotlosses, checkpoint_validation, early_stopping]\n","    return model.fit(inputs_train, outputs_train, validation_data=(inputs_validation, outputs_validation), epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0) # verbose=1\n","\n","def predictModel(model, inputs):\n","    return model.predict(inputs)\n","    \n","def evaluateModel(model, inputs, outputs, batch_size=64):\n","    return model.evaluate(inputs, outputs, batch_size=batch_size, verbose=0)\n","    \n","def loadWeights(model, pathFile):\n","    model.load_weights(pathFile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ymM2zyFSlV5g"},"source":["<h5>Parameter Variations</h5>"]},{"cell_type":"code","metadata":{"id":"o8TDC5VdlV5i"},"source":["experiment_by_dataset = [\n","    { \"train\": [\"pvs_1\", \"pvs_3\", \"pvs_4\", \"pvs_6\", \"pvs_7\", \"pvs_9\"], \"test\":  [\"pvs_2\", \"pvs_5\", \"pvs_8\"]},\n","    { \"train\": [\"pvs_1\", \"pvs_2\", \"pvs_3\", \"pvs_7\", \"pvs_8\", \"pvs_9\"], \"test\":  [\"pvs_4\", \"pvs_5\", \"pvs_6\"]},\n","    { \"train\": [\"pvs_1\", \"pvs_2\", \"pvs_4\", \"pvs_6\", \"pvs_8\", \"pvs_9\"], \"test\":  [\"pvs_3\", \"pvs_5\", \"pvs_7\"]}\n","]\n","\n","experiment_by_fields = [\n","    getFields(acc=True,  gyro=False, speed=True, below_suspension=True), # acc_below_suspension\n","    getFields(acc=False, gyro=True,  speed=True, below_suspension=True), # gyro_below_suspension\n","    getFields(acc=True,  gyro=True,  speed=True, below_suspension=True), # acc_gyro_below_suspension\n","    getFields(acc=True,  gyro=False, speed=True, above_suspension=True), # acc_above_suspension\n","    getFields(acc=False, gyro=True,  speed=True, above_suspension=True), # gyro_above_suspension\n","    getFields(acc=True,  gyro=True,  speed=True, above_suspension=True), # acc_gyro_above_suspension\n","    getFields(acc=True,  gyro=False, speed=True, dashboard=True), # acc_dashboard\n","    getFields(acc=False, gyro=True,  speed=True, dashboard=True), # gyro_dashboard\n","    getFields(acc=True,  gyro=True,  speed=True, dashboard=True) # acc_gyro_dashboard\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E42CTyM2lV5l"},"source":["<h5>Labels Fields</h5>"]},{"cell_type":"code","metadata":{"id":"W9TUEwohlV5m"},"source":["surface_type_labels = [\"dirt_road\", \"cobblestone_road\", \"asphalt_road\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4i3UwyMpGGqo"},"source":["surface_type_labels_plot = [\"Dirt \\n Road\", \"Cobblestone \\n Road\", \"Asphalt \\n Road\"]  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfwQdDlHxkZ_"},"source":["<h5>Execution Log</h5>"]},{"cell_type":"code","metadata":{"id":"hIw1Rq3Z9hnC"},"source":["# Save a log for each experiment execution (params for each execution)\n","def saveExecutionLog(path, data, columns=['experiment', \"window\", \"scaler\", \"input_shape\", \"output_shape\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"]):\n","    save = pd.DataFrame(columns=columns, data=data)\n","    save.to_csv(os.path.join(path, \"experiment-execution-log.csv\"), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2vDUTUC3LbT"},"source":["<h5>Training Functions</h5>"]},{"cell_type":"code","metadata":{"id":"OZ_h99QuxnuW"},"source":["def manageFiles(history, experiment_folder, experiment_file):\n","\n","    test = -1\n","    index = -1\n","    val_acc = -1\n","    \n","    for i in range(0,3):\n","\n","        max_value = max(history[i]['val_acc'])\n","\n","        if max_value > val_acc:\n","            val_acc = max_value\n","            test = i\n","            index = history[i]['val_acc'].index(max_value)\n","            \n","    train_acc = history[test]['acc'][index]\n","    train_loss = history[test]['loss'][index]\n","    val_acc = history[test]['val_acc'][index]\n","    val_loss = history[test]['val_loss'][index]\n","    \n","    test_folder = os.path.join(experiment_folder, \"Test \" + str(test + 1)) \n","    file = modelFileSavedFormat(experiment_file).format(**{'acc': train_acc, 'val_acc': val_acc})\n","\n","    move_from = os.path.join(test_folder, file)\n","    move_to = os.path.join(experiment_folder, file)\n","\n","    shutil.move(move_from, move_to)\n","    \n","    for i in range(0,3):\n","        shutil.rmtree(os.path.join(experiment_folder, \"Test \" + str(i + 1)))\n","\n","    return [train_loss, val_loss, train_acc, val_acc]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ppZlIZI3eLz"},"source":["def getLoadBar():\n","    \n","    global load_bar_main, load_bar_experiment, load_bar_retries, experiment_total_retries\n","    \n","    experiment_total = len(experiment_by_dataset)\n","    experiment_total_iteration = len(input_shapes) * len(scalers)\n","    experiment_total_retries = 3\n","    \n","    load_bar_main = tqdm(total=experiment_total, desc='Main Progress')\n","    load_bar_experiment = tqdm(total=experiment_total_iteration)\n","    load_bar_retries = tqdm(total=experiment_total_retries, desc='Retries')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJS-OGcs4R9O"},"source":["def run(model_fn, patience=10, batch_size=64, epochs=1000, retries=3, experiments=[0,1,2]):\n","\n","    load_bar_main.reset()\n","\n","    for experiment_number in experiments:\n","\n","        load_bar_experiment.reset()\n","        load_bar_experiment.set_description(\"Experiment \" + str(experiment_number + 1))\n","\n","        sets_train = experiment_by_dataset[experiment_number]['train']\n","        sets_test = experiment_by_dataset[experiment_number]['test']\n","\n","        execution_log = []\n","\n","        for input_shape, window_size in input_shapes:\n","\n","            model_args = parameters(input_shape, output_shape)\n","\n","            for scaler_function, scaler_args, scaler_name in scalers:\n","\n","                history = []\n","                scaler, normalized_sets = scaler_function(subsets) if scaler_args is None else scaler_function(subsets, scaler_args) \n","                input_train, input_test, output_train, output_test = getTrainTestSets(normalized_sets, sets_train, sets_test, input_shape, moving_window, sides, mode_label)\n","\n","                if useTfV2:\n","                    input_train = np.array(input_train)\n","                    input_test = np.array(input_test)\n","                    output_train = np.array(output_train)\n","                    output_test = np.array(output_test)\n","                else:\n","                    input_train = [input_train]\n","                    input_test = [input_test]\n","                    output_train = [output_train]\n","                    output_test = [output_test]\n","\n","                for test in range(0, retries):\n","\n","                    model, model_name = model_fn(**model_args)\n","\n","                    experiment_folder = os.path.join(work_folder, model_name, \"Experiment \" + str(experiment_number + 1))\n","                    test_folder = os.path.join(experiment_folder, \"Test \" + str(test + 1))\n","                    diagram_file = \"experiment-\" + str(experiment_number + 1) + \"-window-\" + str(window_size)\n","                    experiment_file = diagram_file + \"-scaler-\" + scaler_name + (\"\" if scaler_args is None else \"-\" + str(scaler_args))\n","\n","                    saveModelDiagram(model, experiment_folder, diagram_file)\n","                    hist = fitModel(model, input_train, output_train, input_test, output_test, test_folder, experiment_file, patience=patience, batch_size=batch_size, epochs=epochs)\n","\n","                    history.append(hist.history)\n","                    load_bar_retries.update(1)\n","\n","                    # Clean Memory\n","                    model, hist = [None, None]\n","                    \n","                    if not(output is None):\n","                        output.clear()\n","\n","                    gc.collect()\n","\n","                metrics = manageFiles(history, experiment_folder, experiment_file)\n","                \n","                execution_log.append([\n","                    experiment_number + 1, \n","                    window_size, \n","                    scaler_name + (\"\" if scaler_args is None else \" \" + str(scaler_args)), \n","                    str(input_shape), \n","                    str(output_shape)\n","                ] + metrics)\n","\n","                saveExecutionLog(experiment_folder, execution_log)\n","                load_bar_experiment.update(1)\n","\n","                # Clean\n","                history, scaler, normalized_sets = [None, None, None]\n","                input_train, input_test, output_train, output_test = [None, None, None, None]\n","                load_bar_retries.reset()\n","                gc.collect()\n","\n","        load_bar_main.update(1)       "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-d9MBAlczYj2"},"source":["def confusionMatrix(files, title, model_fn, input_shape, output_shape, scaler_function, scaler_args):\n","\n","    matrix = []\n","\n","    for experiment_number in range(0,3):\n","\n","        sets_train = experiment_by_dataset[experiment_number]['train']\n","        sets_test = experiment_by_dataset[experiment_number]['test']\n","\n","        scaler, normalized_sets = scaler_function(subsets, scaler_args) \n","        input_train, input_test, output_train, output_test = getTrainTestSets(normalized_sets, sets_train, sets_test, input_shape, moving_window, sides, mode_label)\n","\n","        input_train = np.array(input_train)\n","        input_test = np.array(input_test)\n","        output_train = np.array(output_train)\n","        output_test = np.array(output_test)\n","\n","        model_args = parameters(input_shape, output_shape)\n","        model, model_name = model_fn(**model_args)\n","        loadWeights(model, os.path.join(work_folder, model_name, \"Experiment \" + str(experiment_number + 1), files[experiment_number]))\n","        predictions = predictModel(model, input_test)\n","        print(accuracy_score(output_test.argmax(axis=1), predictions.argmax(axis=1)))\n","        matrix.append(confusion_matrix(output_test.argmax(axis=1), predictions.argmax(axis=1)))\n","\n","    values = (matrix[0] + matrix[1] + matrix[2])\n","    con_mat_df = pd.DataFrame(values, index=surface_type_labels_plot, columns=surface_type_labels_plot)\n","    figure = plt.figure(figsize=(4,4))\n","    sns.set(font_scale=1.2)\n","    sns.heatmap(con_mat_df, annot=True, cmap=plt.cm.Blues, fmt='g', annot_kws={\"size\": 14})\n","    plt.tight_layout()\n","    plt.title(title)\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","    figure.savefig('confusion_matrix.png', bbox_inches=\"tight\")\n","    # plt.savefig('results.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9J8FIHiA3pO"},"source":["def classificationReport(files, model_fn, input_shape, output_shape, scaler_function, scaler_args):\n","\n","    output_values = []\n","    predicted_values = []\n","\n","    for experiment_number in range(0,3):\n","\n","        sets_train = experiment_by_dataset[experiment_number]['train']\n","        sets_test = experiment_by_dataset[experiment_number]['test']\n","\n","        scaler, normalized_sets = scaler_function(subsets, scaler_args) \n","        input_train, input_test, output_train, output_test = getTrainTestSets(normalized_sets, sets_train, sets_test, input_shape, moving_window, sides, mode_label)\n","\n","        input_train = np.array(input_train)\n","        input_test = np.array(input_test)\n","        output_train = np.array(output_train)\n","        output_test = np.array(output_test)\n","\n","        model_args = parameters(input_shape, output_shape)\n","        model, model_name = model_fn(**model_args)\n","        loadWeights(model, os.path.join(work_folder, model_name, \"Experiment \" + str(experiment_number + 1), files[experiment_number]))\n","        predictions = predictModel(model, input_test)\n","\n","        for line in output_test:\n","            output_values.append(line)\n","\n","        for line in predictions:\n","            predicted_values.append(line)\n","\n","        del input_train, input_test, output_train, output_test\n","\n","    return classification_report(y_true = np.array(output_values).argmax(axis=1), y_pred = np.array(predicted_values).argmax(axis=1), output_dict=True, target_names=surface_type_labels)"],"execution_count":null,"outputs":[]}]}